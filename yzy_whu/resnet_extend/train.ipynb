{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "Mindspore-python3.7-aarch64", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "code", "source": "# Copyright 2020 Huawei Technologies Co., Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\"\"\"train resnet.\"\"\"\nimport os\nimport argparse\nimport ast\nfrom mindspore import context\nfrom mindspore import Tensor\nfrom mindspore.nn.optim.momentum import Momentum\nfrom mindspore.train.model import Model\nfrom mindspore.context import ParallelMode\nfrom mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\nfrom mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\nfrom mindspore.train.loss_scale_manager import FixedLossScaleManager\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\nfrom mindspore.communication.management import init, get_rank, get_group_size\nfrom mindspore.common import set_seed\nfrom mindspore.parallel import set_algo_parameters\nimport mindspore.nn as nn\nimport mindspore.common.initializer as weight_init\nfrom src.lr_generator import get_lr, warmup_cosine_annealing_lr\nfrom src.CrossEntropySmooth import CrossEntropySmooth\n\nparser = argparse.ArgumentParser(description='Image classification')\nparser.add_argument('--net', type=str, default=\"resnet18\", help='Resnet Model, either resnet50 or resnet101')\nparser.add_argument('--dataset', type=str, default=\"cifar10\", help='Dataset, either cifar10 or imagenet2012')\nparser.add_argument('--run_distribute', type=ast.literal_eval, default=False, help='Run distribute')\nparser.add_argument('--device_num', type=int, default=1, help='Device num.')\n\nparser.add_argument('--dataset_path', type=str, default=\"./data/cifar-10-batches-bin\", help='Dataset path')\nparser.add_argument('--device_target', type=str, default='Ascend', choices=(\"Ascend\", \"GPU\", \"CPU\"),\n                    help=\"Device target, support Ascend, GPU and CPU.\")\nparser.add_argument('--pre_trained', type=str, default=None, help='Pretrained checkpoint path')\nparser.add_argument('--parameter_server', type=ast.literal_eval, default=False, help='Run parameter server train')\n\n#putin args here\nargs_opt = parser.parse_args(args=[])\n\nset_seed(1)\nif args_opt.net == \"resnet18\":\n    from src.resnet import resnet18 as resnet\n    if args_opt.dataset == \"cifar10\":\n        from src.config import config1 as config\n        from src.dataset import create_dataset1 as create_dataset\n    else:\n        from src.config import config2 as config\n        from src.dataset import create_dataset2 as create_dataset\nelif args_opt.net == \"resnet50\":\n    from src.resnet import resnet50 as resnet\n    if args_opt.dataset == \"cifar10\":\n        from src.config import config1 as config\n        from src.dataset import create_dataset1 as create_dataset\n    else:\n        from src.config import config2 as config\n        from src.dataset import create_dataset2 as create_dataset\nelif args_opt.net == \"resnet101\":\n    from src.resnet import resnet101 as resnet\n    if args_opt.dataset == 'imagenet2012':\n        from src.config import config3 as config\n        from src.dataset import create_dataset3 as create_dataset\nelif args_opt.net == \"se_resnet50\":\n    from src.resnet import se_resnet50 as resnet\n    from src.config import config4 as config\n    from src.dataset import create_dataset4 as create_dataset\n    # change!\u8fd9\u91cc\u6539\u53d8\u4e86\uff0c\u65b0\u589e\u4e00\u4e2aresent152\u6a21\u5f0f\nelif args_opt.net == \"resnet152\":\n    from src.resnet import resnet152 as resnet\n    from src.config import config5 as config\n    from src.dataset import create_dataset3 as create_dataset\n\n\n\n\n\n\nif __name__ == '__main__':\n    target = args_opt.device_target\n    if target == \"CPU\":\n        args_opt.run_distribute = False\n\n    ckpt_save_dir = config.save_checkpoint_path\n\n    # init context\n    context.set_context(mode=context.GRAPH_MODE, device_target=target, save_graphs=False)\n    if args_opt.parameter_server:\n        context.set_ps_context(enable_ps=True)\n    if args_opt.run_distribute:\n        if target == \"Ascend\":\n            device_id = int(os.getenv('DEVICE_ID'))\n            context.set_context(device_id=device_id, enable_auto_mixed_precision=True)\n            context.set_auto_parallel_context(device_num=args_opt.device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n                                              gradients_mean=True)\n            set_algo_parameters(elementwise_op_strategy_follow=True)\n            if args_opt.net == \"resnet50\" or args_opt.net == \"se-resnet50\" or args_opt.net == \"resnet18\":\n                context.set_auto_parallel_context(all_reduce_fusion_config=[85, 160])\n            else:\n                context.set_auto_parallel_context(all_reduce_fusion_config=[180, 313])\n            init()\n        # GPU target\n        else:\n            init()\n            context.set_auto_parallel_context(device_num=get_group_size(), parallel_mode=ParallelMode.DATA_PARALLEL,\n                                              gradients_mean=True)\n            if args_opt.net == \"resnet50\" or args_opt.net == \"resnet18\":\n                context.set_auto_parallel_context(all_reduce_fusion_config=[85, 160])\n        ckpt_save_dir = config.save_checkpoint_path + \"ckpt_\" + str(get_rank()) + \"/\"\n\n    # create dataset\n    dataset = create_dataset(dataset_path=args_opt.dataset_path, do_train=True, repeat_num=1,\n                             batch_size=config.batch_size, target=target, distribute=args_opt.run_distribute)\n    step_size = dataset.get_dataset_size()\n\n    # define net\n    net = resnet(class_num=config.class_num)\n    if args_opt.parameter_server:\n        net.set_param_ps()\n\n    # init weight\n    if args_opt.pre_trained:\n        param_dict = load_checkpoint(args_opt.pre_trained)\n        load_param_into_net(net, param_dict)\n    else:\n        for _, cell in net.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(weight_init.initializer(weight_init.XavierUniform(),\n                                                             cell.weight.shape,\n                                                             cell.weight.dtype))\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(),\n                                                             cell.weight.shape,\n                                                             cell.weight.dtype))\n\n    # init lr\n    if args_opt.net == \"resnet50\" or args_opt.net == \"se-resnet50\" or args_opt.net == \"resnet18\":\n        lr = get_lr(lr_init=config.lr_init, lr_end=config.lr_end, lr_max=config.lr_max,\n                    warmup_epochs=config.warmup_epochs, total_epochs=config.epoch_size, steps_per_epoch=step_size,\n                    lr_decay_mode=config.lr_decay_mode)\n    else:\n        lr = warmup_cosine_annealing_lr(config.lr, step_size, config.warmup_epochs, config.epoch_size,\n                                        config.pretrain_epoch_size * step_size)\n    lr = Tensor(lr)\n\n    # define opt\n    decayed_params = []\n    no_decayed_params = []\n    for param in net.trainable_params():\n        if 'beta' not in param.name and 'gamma' not in param.name and 'bias' not in param.name:\n            decayed_params.append(param)\n        else:\n            no_decayed_params.append(param)\n\n    group_params = [{'params': decayed_params, 'weight_decay': config.weight_decay},\n                    {'params': no_decayed_params},\n                    {'order_params': net.trainable_params()}]\n    opt = Momentum(group_params, lr, config.momentum, loss_scale=config.loss_scale)\n    # define loss, model\n    if target == \"Ascend\":\n        if args_opt.dataset == \"imagenet2012\":\n            if not config.use_label_smooth:\n                config.label_smooth_factor = 0.0\n            loss = CrossEntropySmooth(sparse=True, reduction=\"mean\",\n                                      smooth_factor=config.label_smooth_factor, num_classes=config.class_num)\n        else:\n            loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n        loss_scale = FixedLossScaleManager(config.loss_scale, drop_overflow_update=False)\n        model = Model(net, loss_fn=loss, optimizer=opt, loss_scale_manager=loss_scale, metrics={'acc'},\n                      amp_level=\"O2\", keep_batchnorm_fp32=False)\n    else:\n        # GPU and CPU target\n        if args_opt.dataset == \"imagenet2012\":\n            if not config.use_label_smooth:\n                config.label_smooth_factor = 0.0\n            loss = CrossEntropySmooth(sparse=True, reduction=\"mean\",\n                                      smooth_factor=config.label_smooth_factor, num_classes=config.class_num)\n        else:\n            loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n\n        if (args_opt.net == \"resnet101\" or args_opt.net == \"resnet50\") and \\\n            not args_opt.parameter_server and target != \"CPU\":\n            opt = Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr, config.momentum, config.weight_decay,\n                           config.loss_scale)\n            loss_scale = FixedLossScaleManager(config.loss_scale, drop_overflow_update=False)\n            # Mixed precision\n            model = Model(net, loss_fn=loss, optimizer=opt, loss_scale_manager=loss_scale, metrics={'acc'},\n                          amp_level=\"O2\", keep_batchnorm_fp32=False)\n        else:\n            ## fp32 training\n            opt = Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr, config.momentum, config.weight_decay)\n            model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc'})\n\n    # define callbacks\n    time_cb = TimeMonitor(data_size=step_size)\n    loss_cb = LossMonitor()\n    cb = [time_cb, loss_cb]\n    if config.save_checkpoint:\n        config_ck = CheckpointConfig(save_checkpoint_steps=config.save_checkpoint_epochs * step_size,\n                                     keep_checkpoint_max=config.keep_checkpoint_max)\n        ckpt_cb = ModelCheckpoint(prefix=\"resnet\", directory=ckpt_save_dir, config=config_ck)\n        cb += [ckpt_cb]\n\n    # train model\n    if args_opt.net == \"se-resnet50\":\n        config.epoch_size = config.train_epoch_size\n    dataset_sink_mode = (not args_opt.parameter_server) and target != \"CPU\"\n    model.train(config.epoch_size - config.pretrain_epoch_size, dataset, callbacks=cb,\n                sink_size=dataset.get_dataset_size(), dataset_sink_mode=dataset_sink_mode)\n", "metadata": {}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "[WARNING] ME(1368:281473274722608,MainProcess):2021-06-29-10:37:05.909.794 [mindspore/_check_version.py:207] MindSpore version 1.1.1 and \"te\" wheel package version 1.0 does not match, reference to the match info on: https://www.mindspore.cn/install\nMindSpore version 1.1.1 and \"topi\" wheel package version 0.6.0 does not match, reference to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1368:281473274722608,MainProcess):2021-06-29-10:37:06.682.388 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n", "output_type": "stream"}, {"name": "stdout", "text": "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\nepoch: 1 step: 1562, loss is 1.2814661\nepoch time: 110228.378 ms, per step time: 70.569 ms\nepoch: 2 step: 1562, loss is 1.4351182\nepoch time: 10702.159 ms, per step time: 6.852 ms\nepoch: 3 step: 1562, loss is 1.0470979\nepoch time: 10707.079 ms, per step time: 6.855 ms\nepoch: 4 step: 1562, loss is 1.1382511\nepoch time: 10735.129 ms, per step time: 6.873 ms\nepoch: 5 step: 1562, loss is 0.6129632\nepoch time: 11201.417 ms, per step time: 7.171 ms\nepoch: 6 step: 1562, loss is 0.7036794\nepoch time: 10696.631 ms, per step time: 6.848 ms\nepoch: 7 step: 1562, loss is 0.76188356\nepoch time: 10693.577 ms, per step time: 6.846 ms\nepoch: 8 step: 1562, loss is 1.1560085\nepoch time: 10718.785 ms, per step time: 6.862 ms\nepoch: 9 step: 1562, loss is 0.43818074\nepoch time: 10695.004 ms, per step time: 6.847 ms\nepoch: 10 step: 1562, loss is 0.5218737\nepoch time: 11064.759 ms, per step time: 7.084 ms\nepoch: 11 step: 1562, loss is 0.5142746\nepoch time: 10695.402 ms, per step time: 6.847 ms\nepoch: 12 step: 1562, loss is 0.6735502\nepoch time: 10696.808 ms, per step time: 6.848 ms\nepoch: 13 step: 1562, loss is 0.3646369\nepoch time: 10692.351 ms, per step time: 6.845 ms\nepoch: 14 step: 1562, loss is 0.40654102\nepoch time: 10696.655 ms, per step time: 6.848 ms\nepoch: 15 step: 1562, loss is 0.8485826\nepoch time: 11275.668 ms, per step time: 7.219 ms\nepoch: 16 step: 1562, loss is 0.4962742\nepoch time: 10695.689 ms, per step time: 6.847 ms\nepoch: 17 step: 1562, loss is 0.44350064\nepoch time: 10708.119 ms, per step time: 6.855 ms\nepoch: 18 step: 1562, loss is 0.14816093\nepoch time: 10698.092 ms, per step time: 6.849 ms\nepoch: 19 step: 1562, loss is 0.30806258\nepoch time: 10706.497 ms, per step time: 6.854 ms\nepoch: 20 step: 1562, loss is 0.42414632\nepoch time: 11099.882 ms, per step time: 7.106 ms\nepoch: 21 step: 1562, loss is 0.34307015\nepoch time: 10694.370 ms, per step time: 6.847 ms\nepoch: 22 step: 1562, loss is 0.47495323\nepoch time: 10699.839 ms, per step time: 6.850 ms\nepoch: 23 step: 1562, loss is 0.37446997\nepoch time: 10721.230 ms, per step time: 6.864 ms\nepoch: 24 step: 1562, loss is 0.18980429\nepoch time: 10692.938 ms, per step time: 6.846 ms\nepoch: 25 step: 1562, loss is 0.26427558\nepoch time: 10979.867 ms, per step time: 7.029 ms\nepoch: 26 step: 1562, loss is 0.42270198\nepoch time: 10695.132 ms, per step time: 6.847 ms\nepoch: 27 step: 1562, loss is 0.07254326\nepoch time: 10715.345 ms, per step time: 6.860 ms\nepoch: 28 step: 1562, loss is 0.35288\nepoch time: 10703.146 ms, per step time: 6.852 ms\nepoch: 29 step: 1562, loss is 0.3233958\nepoch time: 10714.858 ms, per step time: 6.860 ms\nepoch: 30 step: 1562, loss is 0.3787933\nepoch time: 11030.791 ms, per step time: 7.062 ms\nepoch: 31 step: 1562, loss is 0.21539637\nepoch time: 10696.569 ms, per step time: 6.848 ms\nepoch: 32 step: 1562, loss is 0.23623854\nepoch time: 10718.571 ms, per step time: 6.862 ms\nepoch: 33 step: 1562, loss is 0.11544999\nepoch time: 10698.223 ms, per step time: 6.849 ms\nepoch: 34 step: 1562, loss is 0.15846883\nepoch time: 10699.949 ms, per step time: 6.850 ms\nepoch: 35 step: 1562, loss is 0.4425542\nepoch time: 11053.889 ms, per step time: 7.077 ms\nepoch: 36 step: 1562, loss is 0.2192029\nepoch time: 10695.577 ms, per step time: 6.847 ms\nepoch: 37 step: 1562, loss is 0.09493863\nepoch time: 10728.925 ms, per step time: 6.869 ms\nepoch: 38 step: 1562, loss is 0.17162728\nepoch time: 10697.232 ms, per step time: 6.848 ms\nepoch: 39 step: 1562, loss is 0.19393346\nepoch time: 10694.654 ms, per step time: 6.847 ms\nepoch: 40 step: 1562, loss is 0.058027156\nepoch time: 11046.172 ms, per step time: 7.072 ms\nepoch: 41 step: 1562, loss is 0.4231249\nepoch time: 10692.774 ms, per step time: 6.846 ms\nepoch: 42 step: 1562, loss is 0.237053\nepoch time: 10693.580 ms, per step time: 6.846 ms\nepoch: 43 step: 1562, loss is 0.07124147\nepoch time: 10705.449 ms, per step time: 6.854 ms\nepoch: 44 step: 1562, loss is 0.35549286\nepoch time: 10701.110 ms, per step time: 6.851 ms\nepoch: 45 step: 1562, loss is 0.11574444\nepoch time: 11263.782 ms, per step time: 7.211 ms\nepoch: 46 step: 1562, loss is 0.17831731\nepoch time: 10693.467 ms, per step time: 6.846 ms\nepoch: 47 step: 1562, loss is 0.26164603\nepoch time: 10693.002 ms, per step time: 6.846 ms\nepoch: 48 step: 1562, loss is 0.21679673\nepoch time: 10695.637 ms, per step time: 6.847 ms\nepoch: 49 step: 1562, loss is 0.05731494\nepoch time: 10703.829 ms, per step time: 6.853 ms\nepoch: 50 step: 1562, loss is 0.05526046\nepoch time: 11063.668 ms, per step time: 7.083 ms\nepoch: 51 step: 1562, loss is 0.1948548\nepoch time: 10693.340 ms, per step time: 6.846 ms\nepoch: 52 step: 1562, loss is 0.024750026\nepoch time: 10696.700 ms, per step time: 6.848 ms\nepoch: 53 step: 1562, loss is 0.060979024\nepoch time: 10710.421 ms, per step time: 6.857 ms\nepoch: 54 step: 1562, loss is 0.13864379\nepoch time: 10699.729 ms, per step time: 6.850 ms\nepoch: 55 step: 1562, loss is 0.06778393\nepoch time: 11024.809 ms, per step time: 7.058 ms\nepoch: 56 step: 1562, loss is 0.12753806\nepoch time: 10694.552 ms, per step time: 6.847 ms\nepoch: 57 step: 1562, loss is 0.13767141\nepoch time: 10694.714 ms, per step time: 6.847 ms\nepoch: 58 step: 1562, loss is 0.12331999\nepoch time: 10703.796 ms, per step time: 6.853 ms\nepoch: 59 step: 1562, loss is 0.020053979\nepoch time: 10694.125 ms, per step time: 6.846 ms\nepoch: 60 step: 1562, loss is 0.023452554\nepoch time: 11061.513 ms, per step time: 7.082 ms\nepoch: 61 step: 1562, loss is 0.06810717\nepoch time: 10696.609 ms, per step time: 6.848 ms\nepoch: 62 step: 1562, loss is 0.087858364\nepoch time: 10696.691 ms, per step time: 6.848 ms\nepoch: 63 step: 1562, loss is 0.0958045\nepoch time: 10695.457 ms, per step time: 6.847 ms\nepoch: 64 step: 1562, loss is 0.019397922\nepoch time: 10703.312 ms, per step time: 6.852 ms\nepoch: 65 step: 1562, loss is 0.013398929\nepoch time: 11055.549 ms, per step time: 7.078 ms\nepoch: 66 step: 1562, loss is 0.005553469\nepoch time: 10695.894 ms, per step time: 6.848 ms\nepoch: 67 step: 1562, loss is 0.010454231\nepoch time: 10709.317 ms, per step time: 6.856 ms\nepoch: 68 step: 1562, loss is 0.009711375\nepoch time: 10731.772 ms, per step time: 6.871 ms\nepoch: 69 step: 1562, loss is 0.017755829\nepoch time: 10703.198 ms, per step time: 6.852 ms\nepoch: 70 step: 1562, loss is 0.0018817808\nepoch time: 11044.037 ms, per step time: 7.070 ms\nepoch: 71 step: 1562, loss is 0.00688272\nepoch time: 10704.421 ms, per step time: 6.853 ms\nepoch: 72 step: 1562, loss is 0.002086711\nepoch time: 10696.717 ms, per step time: 6.848 ms\nepoch: 73 step: 1562, loss is 0.009240551\nepoch time: 10701.706 ms, per step time: 6.851 ms\nepoch: 74 step: 1562, loss is 0.010971217\nepoch time: 10704.893 ms, per step time: 6.853 ms\nepoch: 75 step: 1562, loss is 0.003294669\nepoch time: 11051.526 ms, per step time: 7.075 ms\nepoch: 76 step: 1562, loss is 0.01420134\nepoch time: 10686.765 ms, per step time: 6.842 ms\nepoch: 77 step: 1562, loss is 0.0027168067\nepoch time: 10691.264 ms, per step time: 6.845 ms\nepoch: 78 step: 1562, loss is 0.0050083064\nepoch time: 10694.122 ms, per step time: 6.846 ms\nepoch: 79 step: 1562, loss is 0.011888651\nepoch time: 10703.108 ms, per step time: 6.852 ms\nepoch: 80 step: 1562, loss is 0.012999797\nepoch time: 11062.246 ms, per step time: 7.082 ms\nepoch: 81 step: 1562, loss is 0.001328673\nepoch time: 10692.651 ms, per step time: 6.845 ms\nepoch: 82 step: 1562, loss is 0.013563799\nepoch time: 10692.806 ms, per step time: 6.846 ms\nepoch: 83 step: 1562, loss is 0.005442179\nepoch time: 10705.645 ms, per step time: 6.854 ms\nepoch: 84 step: 1562, loss is 0.00082814216\nepoch time: 10701.840 ms, per step time: 6.851 ms\nepoch: 85 step: 1562, loss is 0.0034805103\nepoch time: 10927.348 ms, per step time: 6.996 ms\nepoch: 86 step: 1562, loss is 0.0002800927\nepoch time: 10685.533 ms, per step time: 6.841 ms\nepoch: 87 step: 1562, loss is 0.0011226387\nepoch time: 10700.252 ms, per step time: 6.850 ms\nepoch: 88 step: 1562, loss is 0.0072918544\nepoch time: 10690.893 ms, per step time: 6.844 ms\nepoch: 89 step: 1562, loss is 0.0015422814\nepoch time: 10685.117 ms, per step time: 6.841 ms\nepoch: 90 step: 1562, loss is 0.009549746\nepoch time: 10981.919 ms, per step time: 7.031 ms\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}